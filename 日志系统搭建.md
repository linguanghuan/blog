# 日志系统搭建

## 流程图

```
Possible directions are:
TB - top bottom
BT - bottom top
RL - right left
LR - left right
TD - same as TB
```



```mermaid
graph LR
flume --> kafka
kafka --> logstash
logstash --> es
es --> kibana

```



## 机器规划

zk集群: h102, h103,h104

es集群：h102,h103,h104

kafka集群: h102,h103,h104



部署位置

es kafka 



所有机器新建app用户，将flume kafka等部署到app中去

```
[root@h101 ~]# cexec useradd app
************************* h *************************
--------- h101---------
--------- h102---------
--------- h103---------
--------- h104---------
[root@h101 ~]# cexec "echo app| passwd app --stdin"
************************* h *************************
--------- h101---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
--------- h102---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
--------- h103---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
--------- h104---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
```



设置h101到所有机器的免密登录

```
[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h102
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
The authenticity of host 'h102 (192.168.56.102)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h102's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h102'"
and check to make sure that only the key(s) you wanted were added.

[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h103
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
The authenticity of host 'h103 (192.168.56.103)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h103's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h103'"
and check to make sure that only the key(s) you wanted were added.

[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h104
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
The authenticity of host 'h104 (192.168.56.104)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h104's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h104'"
and check to make sure that only the key(s) you wanted were added.
```

验证免密登录

```
[app@h101 ~]$ ssh h102
Last login: Thu Jan 17 11:14:21 2019 from h1
[app@h102 ~]$ exit
登出
Connection to h102 closed.
[app@h101 ~]$ ssh h103
Last login: Thu Jan 17 11:14:29 2019 from h1
[app@h103 ~]$ exit
登出
Connection to h103 closed.
[app@h101 ~]$ ssh h104
Last login: Thu Jan 17 11:14:38 2019 from h1
[app@h104 ~]$ exit
登出
Connection to h104 closed.
[app@h101 ~]$ ssh h101
The authenticity of host 'h101 (192.168.56.101)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'h101,192.168.56.101' (ECDSA) to the list of known hosts.
app@h101's password: 
Last login: Thu Jan 17 11:14:14 2019 from h1
[app@h101 ~]$ exit
登出
Connection to h101 closed.
[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h101
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h101's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h101'"
and check to make sure that only the key(s) you wanted were added.

[app@h101 ~]$ ssh h101
Last login: Thu Jan 17 11:17:33 2019 from h101
[app@h101 ~]$ eixt
-bash: eixt: 未找到命令
[app@h101 ~]$ exit
登出
Connection to h101 closed.
```

.bash_profile .bashrc 设置环境变量

```
export C3_HOME=/opt/c3
export PATH=$C3_HOME/usr/bin:$PATH
export MANPATH=$MANPATH:$C3_HOME/doc/man

export JAVA_HOME=/usr/local/java8
export PATH=$JAVA_HOME/bin:$PATH
```



```
[app@h101 ~]$ cpush .bash_profile /home/app/.bash_profile
-bash: cpush: 未找到命令
[app@h101 ~]$ source .bash_profile
[app@h101 ~]$ cpush .bash_profile /home/app/.bash_profile
[app@h101 ~]$ cpush .bashrc /home/app/.bashrc
[app@h101 ~]$ 
```

验证

```
[app@h101 ~]$ cexec jps
************************* h *************************
--------- h101---------
2030 Jps
--------- h102---------
1604 Jps
--------- h103---------
bash: jps: 未找到命令
--------- h104---------
bash: jps: 未找到命令
[app@h101 ~]$ 
```

h103 h104没有java8的目录

从h101拷贝过去

```
[root@h101 local]# scp -r jdk1.8.0_171/ root@h103:/usr/local/jdk1.8.0_171
[root@h101 local]# scp -r jdk1.8.0_171/ root@h104:/usr/local/jdk1.8.0_171
```

```
[root@h101 local]# cexec "ln -s /usr/local/jdk1.8.0_171 /usr/local/java8"
************************* h *************************
--------- h101---------
--------- h102---------
--------- h103---------
--------- h104---------

[root@h101 local]# cexec "ls -l /usr/local/java8"
************************* h *************************
--------- h101---------
lrwxrwxrwx. 1 root root 13 5月  16 2018 /usr/local/java8 -> jdk1.8.0_171/
--------- h102---------
lrwxrwxrwx. 1 root root 13 5月  16 2018 /usr/local/java8 -> jdk1.8.0_171/
--------- h103---------
lrwxrwxrwx. 1 root root 23 1月  17 11:27 /usr/local/java8 -> /usr/local/jdk1.8.0_171
--------- h104---------
lrwxrwxrwx. 1 root root 23 1月  17 11:27 /usr/local/java8 -> /usr/local/jdk1.8.0_171


[app@h101 ~]$ cexec jps
************************* h *************************
--------- h101---------
2146 Jps
--------- h102---------
1673 Jps
--------- h103---------
1630 Jps
--------- h104---------
1606 Jps

[app@h101 ~]$ cexec java -version
************************* h *************************
--------- h101---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
--------- h102---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
--------- h103---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
--------- h104---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
```





## 搭建flume

[flume安装测试](flume安装测试.md)

## 搭建kafka

[kafka安装测试](kafka安装测试.md)



## flume收集日志到kafka

### 创建kafka topic

```
[app@h104 conf]$ kafka-topics.sh --zookeeper h102:2181,h103:2181,h104:2181 --topic tt_topic --replication-factor 3 --partitions 3 --create
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic "tt_topic".

```

提示说不要带.或者_

那就避免吧，这里要处理的是日志，topic就叫logtopic吧

```
[app@h104 conf]$ kafka-topics.sh --zookeeper h102:2181,h103:2181,h104:2181 --topic logtopic --replication-factor 3 --partitions 3 --create
Created topic "logtopic".

```

查看logtopic属性

```
[app@h104 conf]$ kafka-topics.sh --zookeeper h102:2181,h103:2181,h104:2181 --topic logtopic --describe
Topic:logtopic	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: logtopic	Partition: 0	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: logtopic	Partition: 1	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: logtopic	Partition: 2	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2
```



### 开启kafka消费者

```
[app@h104 ~]$ kafka-console-consumer.sh --bootstrap-server  h103:9092  --from-beginning --topic logtopic
```





### 上传和配置flume

```
[app@h104 ~]$ tar -zxvf apache-flume-1.8.0-bin.tar.gz 

[app@h104 ~]$ ln -s apache-flume-1.8.0-bin/ flume
[app@h104 ~]$ cd flume/conf/

[app@h104 conf]$ cp flume-conf.properties.template flume-conf.properties
[app@h104 conf]$ cp flume-env.sh.template flume-env.sh
```



### 配置flume-env.sh

```
可以设置JAVA_HOME，这里好像也可以直接用环境变量中的，所以实际没有改动这个文件

```



### 配置flume-kafka.properties 

```
ag1.sources=src1 src2
ag1.sinks=sink1
ag1.channels=chn1

ag1.sources.src1.type = exec
ag1.sources.src1.shell = /bin/bash -c
ag1.sources.src1.command = tail -F /home/app/kafka/logs/server.log
ag1.sources.src1.channels = chn1

ag1.sources.src2.type = exec
ag1.sources.src2.shell = /bin/bash -c
ag1.sources.src2.command = tail -F /home/app/kafka/logs/server.log.2019-01-17-12
ag1.sources.src2.channels = chn1

ag1.channels.chn1.type = memory
agent.channels.chn1.keep-alive = 60  
ag1.channels.chn1.capacity = 1000
ag1.channels.chn1.transactionCapacity = 100


ag1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink
ag1.sinks.sink1.topic = logtopic
ag1.sinks.sink1.brokerList = h101:9092,h102:9092,h103:9092
ag1.sinks.sink1.channel = chn1


```



### 开启flume收集日志

```
[app@h104 flume]$ bin/flume-ng agent -n ag1 --conf ./conf/ -f conf/flume-kafka.properties  -Dflume.root.logger=DEBUG,console

```



启动后，kafka-consumer就能收到flume收集过来的日志信息了。



### 造数据

收集tmp目录下自己创建的文件，用于观察自己追加过去的内容是否到kafka

```
ag1.sources=src1 src2
ag1.sinks=sink1
ag1.channels=chn1

ag1.sources.src1.type = exec
ag1.sources.src1.shell = /bin/bash -c
ag1.sources.src1.command = tail -F /tmp/server111.log
ag1.sources.src1.channels = chn1

ag1.sources.src2.type = exec
ag1.sources.src2.shell = /bin/bash -c
ag1.sources.src2.command = tail -F /tmp/server222.log
ag1.sources.src2.channels = chn1

ag1.channels.chn1.type = memory
agent.channels.chn1.keep-alive = 60  
ag1.channels.chn1.capacity = 1000
ag1.channels.chn1.transactionCapacity = 100


ag1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink
ag1.sinks.sink1.topic = logtopic
ag1.sinks.sink1.brokerList = h101:9092,h102:9092,h103:9092
ag1.sinks.sink1.channel = chn1

```

造数据

```
[app@h104 ~]$ echo "111111111" >> /tmp/server111.log
[app@h104 ~]$ echo "111111111" >> /tmp/server111.log
[app@h104 ~]$ echo "111111111" >> /tmp/server111.log;echo "222222222222222" >> /tmp/server222.log

```

consomer接收到的数据

```
111111111
111111111
222222222222222
111111111

```

到这一步，说明flume -> kafka 流程测试没问题



## es集群安装

[es集群安装测试](es集群安装测试.md)

这里测试环境，就只启动主节点吧

启动放到后台运行

加上-d参数

```
[app@h101 ~]$ cd es/bin
[app@h101 bin]$ ./elasticsearch -d
[app@h101 bin]$ jps
8569 Kafka
12029 Jps
12015 Elasticsearch
```



## kibana安装



```
[app@h101 ~]$ tar -zxvf kibana-6.4.0-linux-x86_64.tar.gz 
[app@h101 ~]$ rm kibana-6.4.0-linux-x86_64.tar.gz 
[app@h101 ~]$ ln -s kibana-6.4.0-linux-x86_64 kibana

[app@h101 ~]$ cd kibana
[app@h101 kibana]$ vi config/kibana.yml 
```

```
server.host: "192.168.56.101"
```



启动

> [app@h101 kibana]$ bin/kibana



报错

```
  log   [13:37:48.470] [info][listening][server][http] Server running at http://192.168.56.101:5601
  log   [13:37:48.473] [warning][admin][elasticsearch] Unable to revive connection: http://localhost:9200/
  log   [13:37:48.474] [warning][admin][elasticsearch] No living connections
  log   [13:37:48.828] [warning][admin][elasticsearch] Unable to revive connection: http://localhost:9200/
  log   [13:37:48.830] [warning][admin][elasticsearch] No living connections

```

因为es监听，没有监听localhost的

```
tcp6       0      0 192.168.56.101:9200     :::*                    LISTEN      12015/java 
```

再编辑kibana配置文件，设置es的地址

[app@h101 kibana]$ vi config/kibana.yml 

```
elasticsearch.url: "http://192.168.56.101:9200"

```

再启动，启动成功

```
[app@h101 bin]$ ./kibana
  log   [13:40:28.476] [info][status][plugin:kibana@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:28.545] [info][status][plugin:elasticsearch@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.552] [info][status][plugin:xpack_main@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.562] [info][status][plugin:searchprofiler@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.569] [info][status][plugin:ml@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.679] [info][status][plugin:tilemap@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.681] [info][status][plugin:watcher@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.712] [info][status][plugin:license_management@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:28.716] [info][status][plugin:index_management@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:28.975] [info][status][plugin:timelion@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:28.978] [info][status][plugin:graph@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:29.004] [info][status][plugin:monitoring@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:29.008] [warning][security] Generating a random key for xpack.security.encryptionKey. To prevent sessions from being invalidated on restart, please set xpack.security.encryptionKey in kibana.yml
  log   [13:40:29.012] [warning][security] Session cookies will be transmitted over insecure connections. This is not recommended.
  log   [13:40:29.027] [info][status][plugin:security@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:29.063] [info][status][plugin:grokdebugger@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:29.075] [info][status][plugin:dashboard_mode@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:29.090] [info][status][plugin:logstash@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:29.130] [info][status][plugin:apm@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:29.137] [info][status][plugin:console@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:29.141] [info][status][plugin:console_extensions@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:29.150] [info][status][plugin:notifications@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:29.157] [info][status][plugin:metrics@6.4.0] Status changed from uninitialized to green - Ready
  log   [13:40:30.415] [warning][reporting] Generating a random key for xpack.reporting.encryptionKey. To prevent pending reports from failing on restart, please set xpack.reporting.encryptionKey in kibana.yml
  log   [13:40:30.420] [info][status][plugin:reporting@6.4.0] Status changed from uninitialized to yellow - Waiting for Elasticsearch
  log   [13:40:30.524] [info][listening][server][http] Server running at http://192.168.56.101:5601
  log   [13:40:30.616] [info][status][plugin:elasticsearch@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.702] [info][license][xpack] Imported license information from Elasticsearch for the [data] cluster: mode: basic | status: active
  log   [13:40:30.706] [info][status][plugin:xpack_main@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.707] [info][status][plugin:searchprofiler@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.708] [info][status][plugin:ml@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.708] [info][status][plugin:tilemap@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.709] [info][status][plugin:watcher@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.709] [info][status][plugin:index_management@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.710] [info][status][plugin:graph@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.711] [info][status][plugin:grokdebugger@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.716] [info][status][plugin:logstash@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.717] [info][status][plugin:reporting@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.718] [info][kibana-monitoring][monitoring-ui] Starting monitoring stats collection
  log   [13:40:30.724] [info][status][plugin:security@6.4.0] Status changed from yellow to green - Ready
  log   [13:40:30.808] [info][license][xpack] Imported license information from Elasticsearch for the [monitoring] cluster: mode: basic | status: active

```

http://192.168.56.101:5601

可以访问了

http://192.168.56.101:5601/status

查看状态



让kibana运行在h102上吧

```


[app@h101 ~]$ scp -r kibana-6.4.0-linux-x86_64 h102:/home/app/kibana-6.4.0-linux-x86_64
[app@h102 ~]$ ln -s kibana-6.4.0-linux-x86_64 kibana
```





## logstash安装

```
[app@h103 ~]$ tar -zxvf logstash-6.4.0.tar.gz 
[app@h103 ~]$ ln -s logstash-6.4.0 logstash

```



## logstash消费kafka数据到es

```
cd logstash/config
```

> [app@h103 config]$ vi first-pipeline.conf 

```
input {
	kafka {
		bootstrap_servers => "h101:9092"
		topics => "logtopic"
		group_id => "logstash"
	}
}

output {
	elasticsearch {
		hosts => "h101:9200"
		index => "log_index"
	}
}

```

这里kafka和elasticsearch是关键字。对应相应的实现，不能简写成其他字符，比如elasticsearch简写成es

则会报错

```
[2019-01-17T22:49:08,784][ERROR][logstash.plugins.registry] Tried to load a plugin's code, but failed. {:exception=>#<LoadError: no such file to load -- logstash/outputs/es>, :path=>"logstash/outputs/es", :type=>"output", :name=>"es"}
```

启动

```
[app@h103 logstash]$ bin/logstash -f config/first-pipeline.conf 

```

启动成功，日志

```
[app@h103 logstash]$ bin/logstash -f config/first-pipeline.conf 
Sending Logstash logs to /home/app/logstash/logs which is now configured via log4j2.properties
[2019-01-17T22:54:49,678][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2019-01-17T22:54:51,555][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.4.0"}
[2019-01-17T22:54:56,668][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>1, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2019-01-17T22:54:57,992][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://h101:9200/]}}
[2019-01-17T22:54:58,019][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://h101:9200/, :path=>"/"}
[2019-01-17T22:54:58,583][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://h101:9200/"}
[2019-01-17T22:54:58,732][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2019-01-17T22:54:58,737][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2019-01-17T22:54:58,832][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//h101:9200"]}
[2019-01-17T22:54:59,091][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2019-01-17T22:54:59,097][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x1ec3bc87 run>"}
[2019-01-17T22:54:59,239][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2019-01-17T22:54:59,395][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2019-01-17T22:54:59,498][INFO ][logstash.outputs.elasticsearch] Installing elasticsearch template to _template/logstash
[2019-01-17T22:54:59,605][INFO ][org.apache.kafka.clients.consumer.ConsumerConfig] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [h101:9092]
	check.crcs = true
	client.id = logstash-0
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logstash
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2019-01-17T22:54:59,881][INFO ][org.apache.kafka.common.utils.AppInfoParser] Kafka version : 1.1.0
[2019-01-17T22:54:59,881][INFO ][org.apache.kafka.common.utils.AppInfoParser] Kafka commitId : fdcf75ea326b8e07
[2019-01-17T22:55:00,448][INFO ][org.apache.kafka.clients.Metadata] Cluster ID: XmuPMhe6SeSDdDa14uJ8Iw
[2019-01-17T22:55:00,554][INFO ][org.apache.kafka.clients.consumer.internals.AbstractCoordinator] [Consumer clientId=logstash-0, groupId=logstash] Discovered group coordinator h103:9092 (id: 2147483644 rack: null)
[2019-01-17T22:55:00,585][INFO ][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [Consumer clientId=logstash-0, groupId=logstash] Revoking previously assigned partitions []
[2019-01-17T22:55:00,585][INFO ][org.apache.kafka.clients.consumer.internals.AbstractCoordinator] [Consumer clientId=logstash-0, groupId=logstash] (Re-)joining group
[2019-01-17T22:55:00,716][INFO ][org.apache.kafka.clients.consumer.internals.AbstractCoordinator] [Consumer clientId=logstash-0, groupId=logstash] Successfully joined group with generation 1
[2019-01-17T22:55:00,718][INFO ][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [Consumer clientId=logstash-0, groupId=logstash] Setting newly assigned partitions [logtopic-0, logtopic-2, logtopic-1]
[2019-01-17T22:55:00,789][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2019-01-17T22:55:00,863][INFO ][org.apache.kafka.clients.consumer.internals.Fetcher] [Consumer clientId=logstash-0, groupId=logstash] Resetting offset for partition logtopic-2 to offset 21.
[2019-01-17T22:55:00,863][INFO ][org.apache.kafka.clients.consumer.internals.Fetcher] [Consumer clientId=logstash-0, groupId=logstash] Resetting offset for partition logtopic-0 to offset 22.
[2019-01-17T22:55:00,863][INFO ][org.apache.kafka.clients.consumer.internals.Fetcher] [Consumer clientId=logstash-0, groupId=logstash] Resetting offset for partition logtopic-1 to offset 20.

```

看日志，有在消费kafka的日志了。但是没看到有把数据传到es

es查看索引

```
http://h101:9200/_cat/indices?v
```

结果，只有一个.kibana的

```
health status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   .kibana 733ZaDzCRReS_x6KYy9tNA   1   0          1            0        4kb            4kb
```



h104 flume启动收集数据

```
bin/flume-ng agent -n ag1 --conf ./conf/ -f conf/flume-kafka.properties  -Dflume.root.logger=DEBUG,console
```

追加数据到日志

```
[app@h104 ~]$ echo "666666666666666666666666" >> /tmp/server111.log 

```

kafka日志显示有接收到数据

```
2019-01-17 23:09:44,228 (SinkRunner-PollingRunner-DefaultSinkProcessor) [DEBUG - org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:199)] event #1
2019-01-17 23:09:44,234 (kafka-producer-network-thread | producer-1) [DEBUG - org.apache.flume.sink.kafka.SinkCallback.onCompletion(KafkaSink.java:475)] Acked message partition:1 ofset:35
2019-01-17 23:09:44,234 (kafka-producer-network-thread | producer-1) [DEBUG - org.apache.flume.sink.kafka.SinkCallback.onCompletion(KafkaSink.java:478)] Elapsed time for send: 6
2019-01-17 23:09:45,019 (conf-file-poller-0) [DEBUG - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:127)] Checking file:conf/flume-kafka.properties for changes

```



kafka-manager 查看logtopic，也能看到以下参数在增加

```
Sum of partition offsets	107
```



logstash日志没有进一步的动静



修改logstash配置文件

主要是elasticsearch加上一个document_type的配置

```
input {
	kafka {
		bootstrap_servers => "h101:9092"
		topics => "logtopic"
		group_id => "logstash"
		auto_offset_reset => "earliest" #自动将偏移重置为最早的偏移量
	}
}

output {
	elasticsearch {
		hosts => "h101:9200"
		index => "log_index"
		document_type => "test"
	}
}

```

再查看索引

```
http://h101:9200/_cat/indices?v
```

就有数据了

docs.count 为46

```
health status index     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   log_index W4nW-eXBSbCnLqkDockuBA   5   1         46            0     69.6kb         69.6kb
green  open   .kibana   733ZaDzCRReS_x6KYy9tNA   1   0          1            0        4kb            4kb
```

http://h101:9200/log_index

```
{
	"log_index": {
		"aliases": {},
		"mappings": {
			"doc": {
				"properties": {
					"@timestamp": {
						"type": "date"
					},
					"@version": {
						"type": "text",
						"fields": {
							"keyword": {
								"type": "keyword",
								"ignore_above": 256
							}
						}
					},
					"message": {
						"type": "text",
						"fields": {
							"keyword": {
								"type": "keyword",
								"ignore_above": 256
							}
						}
					}
				}
			}
		},
		"settings": {
			"index": {
				"creation_date": "1547737538268",
				"number_of_shards": "5",
				"number_of_replicas": "1",
				"uuid": "W4nW-eXBSbCnLqkDockuBA",
				"version": {
					"created": "6040099"
				},
				"provided_name": "log_index"
			}
		}
	}
}
```



```
http://h101:9200/log_index/_search?q=*&pretty
```

能搜索到日志数据了

```
{
	took: 6,
	timed_out: false,
	_shards: {
		total: 5,
		successful: 5,
		skipped: 0,
		failed: 0
	},
	hits: {
		total: 46,
		max_score: 1,
		hits: [{
				_index: "log_index",
				_type: "doc",
				_id: "m1FYXGgBHddlP0BuoKmP",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:05:37.784Z",
					message: "111111111"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "pFFYXGgBHddlP0BuoKmP",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:05:37.814Z",
					message: "222222222222222"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "s1FaXGgBHddlP0BunKka",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:07:48.499Z",
					message: "222222222222222"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "xFFbXGgBHddlP0Bu3Klb",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:09:10.495Z",
					message: "666666666666666666666666"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "xVFcXGgBHddlP0BuXalR",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:09:43.522Z",
					message: "666666666666666666666666"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "xlFcXGgBHddlP0BuYKkI",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:09:44.220Z",
					message: "666666666666666666666666"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "nlFYXGgBHddlP0BuoKmP",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:05:37.784Z",
					message: "222222222222222"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "olFYXGgBHddlP0BuoKmP",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:05:37.814Z",
					message: "111111111"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "qFFYXGgBHddlP0BuoKmP",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:05:37.815Z",
					message: "111111111"
				}
			},
			{
				_index: "log_index",
				_type: "doc",
				_id: "qVFYXGgBHddlP0BuoKmP",
				_score: 1,
				_source: {
					@version: "1",
					@timestamp: "2019-01-17T15:05:37.815Z",
					message: "111111111"
				}
			}
		]
	}
}

```

chrome  可以搜索安装head插件

```
ElasticSearch Head
```





## kibana展示



## 参考

[日志监控平台搭建 关于flume Kafka Elk](https://www.slahser.com/2016/04/21/%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA-%E5%85%B3%E4%BA%8EFlume-Kafka-ELK/)

[ELK 安装部署实战 (最新6.4.0版本)](https://www.cnblogs.com/woodylau/p/elk_latest.html)

[Filebeat+Kafka+Logstash+ElasticSearch+Kibana搭建完整版](https://www.cnblogs.com/jiashengmei/p/8857053.html)

[ELK实时日志分析平台环境部署--完整记录](https://www.cnblogs.com/kevingrace/p/5919021.html)

[日志收集之--将Kafka数据导入elasticsearch](https://www.cnblogs.com/moonandstar08/p/6556899.html)

[【通天塔之日志分析平台】壹 ELK 环境搭建](https://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/)

[【日志处理、监控ELK、Kafka、Flume等相关资料】](https://www.cnblogs.com/junneyang/p/5633495.html)

[统一日志检索部署（es、logstash、kafka、flume）](https://www.cnblogs.com/sailq21/p/9230336.html)

https://mermaidjs.github.io/flowchart.html



