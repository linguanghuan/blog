# 日志系统搭建

## 流程图

```
Possible directions are:
TB - top bottom
BT - bottom top
RL - right left
LR - left right
TD - same as TB
```



```mermaid
graph LR
flume --> kafka
kafka --> logstash
logstash --> es
es --> kibana

```



## 机器规划

zk集群: h102, h103,h104

es集群：h102,h103,h104

kafka集群: h102,h103,h104



部署位置

es kafka 



所有机器新建app用户，将flume kafka等部署到app中去

```
[root@h101 ~]# cexec useradd app
************************* h *************************
--------- h101---------
--------- h102---------
--------- h103---------
--------- h104---------
[root@h101 ~]# cexec "echo app| passwd app --stdin"
************************* h *************************
--------- h101---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
--------- h102---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
--------- h103---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
--------- h104---------
更改用户 app 的密码 。
passwd：所有的身份验证令牌已经成功更新。
```



设置h101到所有机器的免密登录

```
[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h102
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
The authenticity of host 'h102 (192.168.56.102)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h102's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h102'"
and check to make sure that only the key(s) you wanted were added.

[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h103
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
The authenticity of host 'h103 (192.168.56.103)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h103's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h103'"
and check to make sure that only the key(s) you wanted were added.

[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h104
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
The authenticity of host 'h104 (192.168.56.104)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h104's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h104'"
and check to make sure that only the key(s) you wanted were added.
```

验证免密登录

```
[app@h101 ~]$ ssh h102
Last login: Thu Jan 17 11:14:21 2019 from h1
[app@h102 ~]$ exit
登出
Connection to h102 closed.
[app@h101 ~]$ ssh h103
Last login: Thu Jan 17 11:14:29 2019 from h1
[app@h103 ~]$ exit
登出
Connection to h103 closed.
[app@h101 ~]$ ssh h104
Last login: Thu Jan 17 11:14:38 2019 from h1
[app@h104 ~]$ exit
登出
Connection to h104 closed.
[app@h101 ~]$ ssh h101
The authenticity of host 'h101 (192.168.56.101)' can't be established.
ECDSA key fingerprint is SHA256:epP1OFtThjD3D1WojGrAnYkmT09fmRQWs2pdRe/BzcA.
ECDSA key fingerprint is MD5:b8:0f:97:6b:a4:97:d5:a0:1a:dc:6d:2a:dd:26:53:a5.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'h101,192.168.56.101' (ECDSA) to the list of known hosts.
app@h101's password: 
Last login: Thu Jan 17 11:14:14 2019 from h1
[app@h101 ~]$ exit
登出
Connection to h101 closed.
[app@h101 ~]$ ssh-copy-id -i .ssh/id_rsa.pub h101
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
app@h101's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'h101'"
and check to make sure that only the key(s) you wanted were added.

[app@h101 ~]$ ssh h101
Last login: Thu Jan 17 11:17:33 2019 from h101
[app@h101 ~]$ eixt
-bash: eixt: 未找到命令
[app@h101 ~]$ exit
登出
Connection to h101 closed.
```

.bash_profile .bashrc 设置环境变量

```
export C3_HOME=/opt/c3
export PATH=$C3_HOME/usr/bin:$PATH
export MANPATH=$MANPATH:$C3_HOME/doc/man

export JAVA_HOME=/usr/local/java8
export PATH=$JAVA_HOME/bin:$PATH
```



```
[app@h101 ~]$ cpush .bash_profile /home/app/.bash_profile
-bash: cpush: 未找到命令
[app@h101 ~]$ source .bash_profile
[app@h101 ~]$ cpush .bash_profile /home/app/.bash_profile
[app@h101 ~]$ cpush .bashrc /home/app/.bashrc
[app@h101 ~]$ 
```

验证

```
[app@h101 ~]$ cexec jps
************************* h *************************
--------- h101---------
2030 Jps
--------- h102---------
1604 Jps
--------- h103---------
bash: jps: 未找到命令
--------- h104---------
bash: jps: 未找到命令
[app@h101 ~]$ 
```

h103 h104没有java8的目录

从h101拷贝过去

```
[root@h101 local]# scp -r jdk1.8.0_171/ root@h103:/usr/local/jdk1.8.0_171
[root@h101 local]# scp -r jdk1.8.0_171/ root@h104:/usr/local/jdk1.8.0_171
```

```
[root@h101 local]# cexec "ln -s /usr/local/jdk1.8.0_171 /usr/local/java8"
************************* h *************************
--------- h101---------
--------- h102---------
--------- h103---------
--------- h104---------

[root@h101 local]# cexec "ls -l /usr/local/java8"
************************* h *************************
--------- h101---------
lrwxrwxrwx. 1 root root 13 5月  16 2018 /usr/local/java8 -> jdk1.8.0_171/
--------- h102---------
lrwxrwxrwx. 1 root root 13 5月  16 2018 /usr/local/java8 -> jdk1.8.0_171/
--------- h103---------
lrwxrwxrwx. 1 root root 23 1月  17 11:27 /usr/local/java8 -> /usr/local/jdk1.8.0_171
--------- h104---------
lrwxrwxrwx. 1 root root 23 1月  17 11:27 /usr/local/java8 -> /usr/local/jdk1.8.0_171


[app@h101 ~]$ cexec jps
************************* h *************************
--------- h101---------
2146 Jps
--------- h102---------
1673 Jps
--------- h103---------
1630 Jps
--------- h104---------
1606 Jps

[app@h101 ~]$ cexec java -version
************************* h *************************
--------- h101---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
--------- h102---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
--------- h103---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
--------- h104---------
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
```





## 搭建flume

[flume安装测试](flume安装测试.md)

## 搭建kafka

[kafka安装测试](kafka安装测试.md)



## flume收集日志到kafka

### 创建kafka topic

```
[app@h104 conf]$ kafka-topics.sh --zookeeper h102:2181,h103:2181,h104:2181 --topic tt_topic --replication-factor 3 --partitions 3 --create
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic "tt_topic".

```

提示说不要带.或者_

那就避免吧，这里要处理的是日志，topic就叫logtopic吧

```
[app@h104 conf]$ kafka-topics.sh --zookeeper h102:2181,h103:2181,h104:2181 --topic logtopic --replication-factor 3 --partitions 3 --create
Created topic "logtopic".

```

查看logtopic属性

```
[app@h104 conf]$ kafka-topics.sh --zookeeper h102:2181,h103:2181,h104:2181 --topic logtopic --describe
Topic:logtopic	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: logtopic	Partition: 0	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: logtopic	Partition: 1	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: logtopic	Partition: 2	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2
```



### 开启kafka消费者

```
[app@h104 ~]$ kafka-console-consumer.sh --bootstrap-server  h103:9092  --from-beginning --topic logtopic
```





### 上传和配置flume

```
[app@h104 ~]$ tar -zxvf apache-flume-1.8.0-bin.tar.gz 

[app@h104 ~]$ ln -s apache-flume-1.8.0-bin/ flume
[app@h104 ~]$ cd flume/conf/

[app@h104 conf]$ cp flume-conf.properties.template flume-conf.properties
[app@h104 conf]$ cp flume-env.sh.template flume-env.sh
```



### 配置flume-env.sh

```
可以设置JAVA_HOME，这里好像也可以直接用环境变量中的，所以实际没有改动这个文件

```



### 配置flume-kafka.properties 

```
ag1.sources=src1 src2
ag1.sinks=sink1
ag1.channels=chn1

ag1.sources.src1.type = exec
ag1.sources.src1.shell = /bin/bash -c
ag1.sources.src1.command = tail -F /home/app/kafka/logs/server.log
ag1.sources.src1.channels = chn1

ag1.sources.src2.type = exec
ag1.sources.src2.shell = /bin/bash -c
ag1.sources.src2.command = tail -F /home/app/kafka/logs/server.log.2019-01-17-12
ag1.sources.src2.channels = chn1

ag1.channels.chn1.type = memory
agent.channels.chn1.keep-alive = 60  
ag1.channels.chn1.capacity = 1000
ag1.channels.chn1.transactionCapacity = 100


ag1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink
ag1.sinks.sink1.topic = logtopic
ag1.sinks.sink1.brokerList = h101:9092,h102:9092,h103:9092
ag1.sinks.sink1.channel = chn1


```



### 开启flume收集日志

```
[app@h104 flume]$ bin/flume-ng agent -n ag1 --conf ./conf/ -f conf/flume-kafka.properties  -Dflume.root.logger=DEBUG,console

```



启动后，kafka-consumer就能收到flume收集过来的日志信息了。



### 造数据

收集tmp目录下自己创建的文件，用于观察自己追加过去的内容是否到kafka

```
ag1.sources=src1 src2
ag1.sinks=sink1
ag1.channels=chn1

ag1.sources.src1.type = exec
ag1.sources.src1.shell = /bin/bash -c
ag1.sources.src1.command = tail -F /tmp/server111.log
ag1.sources.src1.channels = chn1

ag1.sources.src2.type = exec
ag1.sources.src2.shell = /bin/bash -c
ag1.sources.src2.command = tail -F /tmp/server222.log
ag1.sources.src2.channels = chn1

ag1.channels.chn1.type = memory
agent.channels.chn1.keep-alive = 60  
ag1.channels.chn1.capacity = 1000
ag1.channels.chn1.transactionCapacity = 100


ag1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink
ag1.sinks.sink1.topic = logtopic
ag1.sinks.sink1.brokerList = h101:9092,h102:9092,h103:9092
ag1.sinks.sink1.channel = chn1

```

造数据

```
[app@h104 ~]$ echo "111111111" >> /tmp/server111.log
[app@h104 ~]$ echo "111111111" >> /tmp/server111.log
[app@h104 ~]$ echo "111111111" >> /tmp/server111.log;echo "222222222222222" >> /tmp/server222.log

```

consomer接收到的数据

```
111111111
111111111
222222222222222
111111111

```

到这一步，说明flume -> kafka 流程测试没问题



## logstash消费kafka数据到es



## kibana展示



## 参考

[日志监控平台搭建 关于flume Kafka Elk](https://www.slahser.com/2016/04/21/%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA-%E5%85%B3%E4%BA%8EFlume-Kafka-ELK/)

[ELK 安装部署实战 (最新6.4.0版本)](https://www.cnblogs.com/woodylau/p/elk_latest.html)

[日志收集之--将Kafka数据导入elasticsearch](https://www.cnblogs.com/moonandstar08/p/6556899.html)

[【通天塔之日志分析平台】壹 ELK 环境搭建](https://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/)

[【日志处理、监控ELK、Kafka、Flume等相关资料】](https://www.cnblogs.com/junneyang/p/5633495.html)

[统一日志检索部署（es、logstash、kafka、flume）](https://www.cnblogs.com/sailq21/p/9230336.html)

https://mermaidjs.github.io/flowchart.html



